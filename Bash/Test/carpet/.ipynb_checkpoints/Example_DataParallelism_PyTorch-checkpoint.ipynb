{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch : Data Parallelism multi-GPU et multi-nœuds\n",
    "## Mise en pratique\n",
    "\n",
    "*Notebook rédigé par l'équipe assistance IA de l'IDRIS, novembre 2020*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce document présente la méthode à adopter sur Jean Zay pour distribuer votre entraînement PyTorch selon la méthode du ***Data Parallelism***. Il prend comme référence la [documentation pytorch](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) et illustre la [documentation IDRIS](http://www.idris.fr/jean-zay/gpu/jean-zay-gpu-torch-multi.html).\n",
    "\n",
    "Dans l'exemple proposé, nous entraînons un réseau de neurones convolutionnel sur la base de données MNIST. L'apprentissage s'exécute sur plusieurs GPU et plusieurs nœuds de calcul Jean Zay.\n",
    "\n",
    "Il s'agit ici de :\n",
    "* préparer la base de données MNIST\n",
    "* rédiger le script Python pour l'apprentissage distribué (Data Parallelism)\n",
    "* réaliser une exécution parallèle sur Jean Zay\n",
    "\n",
    "Il est à noter que les données MNIST et le modèle utilisé dans cet exemple sont très simples. Cela permet de présenter un code court et de tester rapidement la configuration du *Data Parallelism*, mais pas de mesurer une accélération de l'apprentissage. En effet, les temps de transfert entre GPU et le temps d'initialisation des *kernels* GPU ne sont pas négligeables par rapport aux temps d'exécution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environnement de calcul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook est prévu pour être exécuté à partir d'une machine frontale de Jean-Zay. Le *hostname* doit être jean-zay[1-5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jean-zay2\r\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un module PyTorch doit avoir été chargé pour le bon fonctionnement de ce Notebook. Par exemple, le module `pytorch-gpu/py3/1.7.0` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1h\u001b=\r",
      "Currently Loaded Modulefiles:\u001b[m\r\n",
      " 1) gcc/8.3.1           4) cudnn/8.0.4.30-cuda-10.2   7) \u001b[4mopenmpi/4.0.5-cuda\u001b[0m     \u001b[m\r\n",
      " 2) cuda/10.2           5) intel-mkl/2020.4           8) pytorch-gpu/py3/1.8.0  \u001b[m\r\n",
      " 3) nccl/2.8.3-1-cuda   6) magma/2.5.4-cuda          \u001b[m\r\n",
      "\r",
      "\u001b[K\u001b[?1l\u001b>"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un dossier `checkpoint` si il n'existe pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoint’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoint\n",
    "!rm checkpoint/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation de la base de données MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base de données MNIST est disponible sur Jean Zay dans le DSDIR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque** : le DSDIR, comme le SCRATCH, est un espace disque GPFS dont la bande passante est d'environ 300 Go/s en écriture et en lecture. Ils sont à privilégier pour les codes ayant une utilisation intense des opérations d'entrées/sorties. Votre space personnel SCRATCH est dédié à vos bases privées et l'espace commun DSDIR comprend la plupart des bases publiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez tester l'accès aux données avec la commande ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: /gpfsdswork/dataset\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torchvision.datasets.MNIST(root=os.environ['DSDIR'],\n",
    "                            train=True,\n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rédaction du script Python pour l'apprentissage distribué (Data Parallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette section, nous rédigeons le script Python d'entraînement dans le fichier 'mnist-distributed.py'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Chargement des librairies et définition de la fonction *main* :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mnist-distributed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mnist-distributed.py \n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import argparse\n",
    "import torch.multiprocessing as mp\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import idr_torch\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-b', '--batch-size', default=128, type =int,\n",
    "                        help='batch size. it will be divided in mini-batch for each worker')\n",
    "    parser.add_argument('-e','--epochs', default=2, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('-c','--checkpoint', default=None, type=str,\n",
    "                        help='path to checkpoint to load')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train(args)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Création du modèle d'apprentissage (réseau de neurones convulationnel simple à 2 couches) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to mnist-distributed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a mnist-distributed.py\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Définition de la fonction d'apprentissage distribué (les *timers* et les affichages sont gérés par le *process* 0, qui est le *process* maître)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to mnist-distributed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a mnist-distributed.py\n",
    "\n",
    "def train(args):\n",
    "    \n",
    "    # configure distribution method: define address and port of the master node and initialise communication backend (NCCL)\n",
    "    dist.init_process_group(backend='nccl', init_method='env://', world_size=idr_torch.size, rank=idr_torch.rank)\n",
    "    \n",
    "    # distribute model\n",
    "    torch.cuda.set_device(idr_torch.local_rank)\n",
    "    gpu = torch.device(\"cuda\")\n",
    "    model = ConvNet().to(gpu)\n",
    "    ddp_model = DistributedDataParallel(model, device_ids=[idr_torch.local_rank])\n",
    "    if args.checkpoint is not None:\n",
    "        map_location = {'cuda:%d' % 0: 'cuda:%d' % idr_torch.local_rank}\n",
    "        ddp_model.load_state_dict(torch.load(args.checkpoint, map_location=map_location))\n",
    "    \n",
    "    # distribute batch size (mini-batch)\n",
    "    batch_size = args.batch_size \n",
    "    batch_size_per_gpu = batch_size // idr_torch.size\n",
    "    \n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  \n",
    "    optimizer = torch.optim.SGD(ddp_model.parameters(), 1e-4)\n",
    "\n",
    "    # load data with distributed sampler\n",
    "    train_dataset = torchvision.datasets.MNIST(root=os.environ['DSDIR'],\n",
    "                                               train=True,\n",
    "                                               transform=transforms.ToTensor(),\n",
    "                                               download=False)\n",
    "    \n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,\n",
    "                                                                    num_replicas=idr_torch.size,\n",
    "                                                                    rank=idr_torch.rank)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                               batch_size=batch_size_per_gpu,\n",
    "                                               shuffle=False,\n",
    "                                               num_workers=0,\n",
    "                                               pin_memory=True,\n",
    "                                               sampler=train_sampler)\n",
    "\n",
    "    # training (timers and display handled by process 0)\n",
    "    if idr_torch.rank == 0: start = datetime.now()         \n",
    "    total_step = len(train_loader)\n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        if idr_torch.rank == 0: start_dataload = time()\n",
    "        \n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            \n",
    "            # distribution of images and labels to all GPUs\n",
    "            images = images.to(gpu, non_blocking=True)\n",
    "            labels = labels.to(gpu, non_blocking=True) \n",
    "            \n",
    "            if idr_torch.rank == 0: stop_dataload = time()\n",
    "\n",
    "            if idr_torch.rank == 0: start_training = time()\n",
    "            \n",
    "            # forward pass\n",
    "            outputs = ddp_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if idr_torch.rank == 0: stop_training = time() \n",
    "            if (i + 1) % 200 == 0 and idr_torch.rank == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Time data load: {:.3f}ms, Time training: {:.3f}ms'.format(epoch + 1, args.epochs,\n",
    "                                                                        i + 1, total_step, loss.item(), (stop_dataload - start_dataload)*1000,\n",
    "                                                                        (stop_training - start_training)*1000))\n",
    "            if idr_torch.rank == 0: start_dataload = time()\n",
    "                    \n",
    "        #Save checkpoint at every end of epoch\n",
    "        if idr_torch.rank == 0:\n",
    "            torch.save(ddp_model.state_dict(), './checkpoint/{}GPU_{}epoch.checkpoint'.format(idr_torch.size, epoch+1))\n",
    "    \n",
    "\n",
    "    if idr_torch.rank == 0:\n",
    "        print(\">>> Training complete in: \" + str(datetime.now() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Définition de la fonction principale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to mnist-distributed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a mnist-distributed.py\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # get distributed configuration from Slurm environment\n",
    "    NODE_ID = os.environ['SLURM_NODEID']\n",
    "    MASTER_ADDR = os.environ['MASTER_ADDR']\n",
    "    \n",
    "    # display info\n",
    "    if idr_torch.rank == 0:\n",
    "        print(\">>> Training on \", len(idr_torch.hostnames), \" nodes and \", idr_torch.size, \" processes, master node is \", MASTER_ADDR)\n",
    "    print(\"- Process {} corresponds to GPU {} of node {}\".format(idr_torch.rank, idr_torch.local_rank, NODE_ID))\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple d'exécution mono-nœud mono-GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Écriture du script batch de soumission\n",
    "\n",
    "**Rappel**:  si votre unique projet dispose d'heures CPU et GPU ou si votre login est rattaché à plusieurs projets, vous devez impérativement préciser l'attribution sur laquelle doit être décomptée les heures consommées par vos calculs, en ajoutant l'option `--account=my_project@gpu` comme indiqué dans la [documentation IDRIS](http://www.idris.fr/jean-zay/cpu/jean-zay-cpu-doc_account.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting batch_monogpu.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile batch_monogpu.slurm\n",
    "#!/bin/sh\n",
    "#SBATCH --job-name=mnist_pytorch_monogpu\n",
    "#SBATCH --output=mnist_pytorch_monogpu.out\n",
    "#SBATCH --error=mnist_pytorch_monogpu.out\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --cpus-per-task=10\n",
    "#SBATCH --hint=nomultithread\n",
    "#SBATCH --time=00:10:00\n",
    "#SBATCH --qos=qos_gpu-dev\n",
    "\n",
    "# go into the submission directory \n",
    "cd ${SLURM_SUBMIT_DIR}\n",
    "\n",
    "# cleans out modules loaded in interactive and inherited by default\n",
    "module purge\n",
    "\n",
    "# loading modules\n",
    "module load pytorch-gpu/py3/1.7.0\n",
    "\n",
    "# echo of launched commands\n",
    "set -x\n",
    "\n",
    "# code execution\n",
    "srun python -u mnist-distributed.py --epochs 8 --batch-size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Soumission du script batch et affichage de la sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 210415\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# submit job\n",
    "sbatch batch_monogpu.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \n",
      "            210415   gpu_p13 mnist_py  ssos040  R       0:49      1 r10i2n5 \n",
      " Done!\n"
     ]
    }
   ],
   "source": [
    "# watch Slurm queue line until the job is done\n",
    "# execution should take about 1 minute\n",
    "import time\n",
    "sq = !squeue -u $USER -n mnist_pytorch_monogpu\n",
    "print(sq[0])\n",
    "while len(sq) >= 2:\n",
    "    print(sq[1],end='\\r')\n",
    "    time.sleep(5)\n",
    "    sq = !squeue -u $USER -n mnist_pytorch_monogpu\n",
    "print('\\n Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pytorch-gpu/py3/1.7.0\r\n",
      "  Loading requirement: gcc/8.3.1 cuda/10.2 nccl/2.6.4-1-cuda\r\n",
      "    cudnn/7.6.5.32-cuda-10.2 intel-mkl/2020.1 magma/2.5.3-cuda\r\n",
      "    openmpi/4.0.2-cuda\r\n",
      "+ srun python -u mnist-distributed.py --epochs 8 --batch-size 128\r\n",
      ">>> Training on  1  nodes and  1  processes, master node is  r10i2n5\r\n",
      "- Process 0 corresponds to GPU 0 of node 0\r\n",
      "Epoch [1/8], Step [200/469], Loss: 2.0360, Time data load: 10.056ms, Time training: 2.348ms\r\n",
      "Epoch [1/8], Step [400/469], Loss: 1.8188, Time data load: 10.068ms, Time training: 2.361ms\r\n",
      "Epoch [2/8], Step [200/469], Loss: 1.5095, Time data load: 10.085ms, Time training: 2.363ms\r\n",
      "Epoch [2/8], Step [400/469], Loss: 1.3832, Time data load: 10.171ms, Time training: 2.345ms\r\n",
      "Epoch [3/8], Step [200/469], Loss: 1.1738, Time data load: 10.049ms, Time training: 2.344ms\r\n",
      "Epoch [3/8], Step [400/469], Loss: 1.1128, Time data load: 10.061ms, Time training: 2.354ms\r\n",
      "Epoch [4/8], Step [200/469], Loss: 0.9567, Time data load: 10.052ms, Time training: 2.342ms\r\n",
      "Epoch [4/8], Step [400/469], Loss: 0.9355, Time data load: 10.035ms, Time training: 2.340ms\r\n",
      "Epoch [5/8], Step [200/469], Loss: 0.8100, Time data load: 10.074ms, Time training: 2.343ms\r\n",
      "Epoch [5/8], Step [400/469], Loss: 0.8117, Time data load: 10.072ms, Time training: 2.368ms\r\n",
      "Epoch [6/8], Step [200/469], Loss: 0.7059, Time data load: 10.554ms, Time training: 2.894ms\r\n",
      "Epoch [6/8], Step [400/469], Loss: 0.7210, Time data load: 10.158ms, Time training: 2.351ms\r\n",
      "Epoch [7/8], Step [200/469], Loss: 0.6291, Time data load: 10.103ms, Time training: 2.343ms\r\n",
      "Epoch [7/8], Step [400/469], Loss: 0.6514, Time data load: 10.107ms, Time training: 2.360ms\r\n",
      "Epoch [8/8], Step [200/469], Loss: 0.5704, Time data load: 10.074ms, Time training: 2.347ms\r\n",
      "Epoch [8/8], Step [400/469], Loss: 0.5961, Time data load: 10.098ms, Time training: 2.353ms\r\n",
      ">>> Training complete in: 0:00:48.187814\r\n"
     ]
    }
   ],
   "source": [
    "# display output\n",
    "%cat mnist_pytorch_monogpu.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple d'exécution mono-nœud multi-GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Écriture du script batch de soumission\n",
    "\n",
    "**Rappel**:  si votre unique projet dispose d'heures CPU et GPU ou si votre login est rattaché à plusieurs projets, vous devez impérativement préciser l'attribution sur laquelle doit être décomptée les heures consommées par vos calculs, en ajoutant l'option `--account=my_project@gpu` comme indiqué dans la [documentation IDRIS](http://www.idris.fr/jean-zay/cpu/jean-zay-cpu-doc_account.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting batch_mononode.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile batch_mononode.slurm\n",
    "#!/bin/sh\n",
    "#SBATCH --job-name=mnist_pytorch_mononode\n",
    "#SBATCH --output=mnist_pytorch_mononode.out\n",
    "#SBATCH --error=mnist_pytorch_mononode.out\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=4\n",
    "#SBATCH --gres=gpu:4\n",
    "#SBATCH --cpus-per-task=10\n",
    "#SBATCH --hint=nomultithread\n",
    "#SBATCH --time=00:10:00\n",
    "#SBATCH --qos=qos_gpu-dev\n",
    "\n",
    "# go into the submission directory \n",
    "cd ${SLURM_SUBMIT_DIR}\n",
    "\n",
    "# cleans out modules loaded in interactive and inherited by default\n",
    "module purge\n",
    "\n",
    "# loading modules\n",
    "module load pytorch-gpu/py3/1.7.0\n",
    "\n",
    "# echo of launched commands\n",
    "set -x\n",
    "\n",
    "# code execution\n",
    "srun python -u mnist-distributed.py --epochs 8 --batch-size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Soumission du script batch et affichage de la sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 210422\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# submit job\n",
    "sbatch batch_mononode.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \n",
      "            210422   gpu_p13 mnist_py  ssos040  R       0:23      1 r10i7n0 \n",
      " Done!\n"
     ]
    }
   ],
   "source": [
    "# watch Slurm queue line until the job is done\n",
    "# execution should take less than 1 minute\n",
    "import time\n",
    "sq = !squeue -u $USER -n mnist_pytorch_mononode\n",
    "print(sq[0])\n",
    "while len(sq) >= 2:\n",
    "    print(sq[1],end='\\r')\n",
    "    time.sleep(5)\n",
    "    sq = !squeue -u $USER -n mnist_pytorch_mononode\n",
    "print('\\n Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pytorch-gpu/py3/1.7.0\r\n",
      "  Loading requirement: gcc/8.3.1 cuda/10.2 nccl/2.6.4-1-cuda\r\n",
      "    cudnn/7.6.5.32-cuda-10.2 intel-mkl/2020.1 magma/2.5.3-cuda\r\n",
      "    openmpi/4.0.2-cuda\r\n",
      "+ srun python -u mnist-distributed.py --epochs 8 --batch-size 128\r\n",
      "- Process 3 corresponds to GPU 3 of node 0\r\n",
      ">>> Training on  1  nodes and  4  processes, master node is  r10i7n0\r\n",
      "- Process 0 corresponds to GPU 0 of node 0\r\n",
      "- Process 1 corresponds to GPU 1 of node 0\r\n",
      "- Process 2 corresponds to GPU 2 of node 0\r\n",
      "Epoch [1/8], Step [200/469], Loss: 2.0835, Time data load: 2.685ms, Time training: 2.338ms\r\n",
      "Epoch [1/8], Step [400/469], Loss: 1.8022, Time data load: 2.705ms, Time training: 2.336ms\r\n",
      "Epoch [2/8], Step [200/469], Loss: 1.5930, Time data load: 2.713ms, Time training: 2.280ms\r\n",
      "Epoch [2/8], Step [400/469], Loss: 1.3419, Time data load: 2.697ms, Time training: 2.327ms\r\n",
      "Epoch [3/8], Step [200/469], Loss: 1.2721, Time data load: 2.682ms, Time training: 2.329ms\r\n",
      "Epoch [3/8], Step [400/469], Loss: 1.0584, Time data load: 2.691ms, Time training: 2.326ms\r\n",
      "Epoch [4/8], Step [200/469], Loss: 1.0477, Time data load: 2.703ms, Time training: 2.319ms\r\n",
      "Epoch [4/8], Step [400/469], Loss: 0.8764, Time data load: 2.707ms, Time training: 2.323ms\r\n",
      "Epoch [5/8], Step [200/469], Loss: 0.8897, Time data load: 2.688ms, Time training: 2.299ms\r\n",
      "Epoch [5/8], Step [400/469], Loss: 0.7532, Time data load: 2.688ms, Time training: 2.317ms\r\n",
      "Epoch [6/8], Step [200/469], Loss: 0.7753, Time data load: 2.710ms, Time training: 2.321ms\r\n",
      "Epoch [6/8], Step [400/469], Loss: 0.6658, Time data load: 2.724ms, Time training: 2.317ms\r\n",
      "Epoch [7/8], Step [200/469], Loss: 0.6900, Time data load: 2.693ms, Time training: 2.321ms\r\n",
      "Epoch [7/8], Step [400/469], Loss: 0.6002, Time data load: 2.702ms, Time training: 2.321ms\r\n",
      "Epoch [8/8], Step [200/469], Loss: 0.6241, Time data load: 2.713ms, Time training: 2.326ms\r\n",
      "Epoch [8/8], Step [400/469], Loss: 0.5487, Time data load: 2.708ms, Time training: 2.321ms\r\n",
      ">>> Training complete in: 0:00:20.342708\r\n"
     ]
    }
   ],
   "source": [
    "#display output \n",
    "%cat mnist_pytorch_mononode.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple d'exécution multi-nœuds multi-GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Écriture du script batch de soumission\n",
    "\n",
    "**Rappel**:  si votre unique projet dispose d'heures CPU et GPU ou si votre login est rattaché à plusieurs projets, vous devez impérativement préciser l'attribution sur laquelle doit être décomptée les heures consommées par vos calculs, en ajoutant l'option `--account=my_project@gpu` comme indiqué dans la [documentation IDRIS](http://www.idris.fr/jean-zay/cpu/jean-zay-cpu-doc_account.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting batch_multinode.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile batch_multinode.slurm\n",
    "#!/bin/sh\n",
    "#SBATCH --job-name=mnist_pytorch_multinode\n",
    "#SBATCH --output=mnist_pytorch_multinode.out\n",
    "#SBATCH --error=mnist_pytorch_multinode.out\n",
    "#SBATCH --nodes=3\n",
    "#SBATCH --gres=gpu:4\n",
    "#SBATCH --ntasks-per-node=4\n",
    "#SBATCH --cpus-per-task=10\n",
    "#SBATCH --hint=nomultithread\n",
    "#SBATCH --time=00:10:00\n",
    "#SBATCH --qos=qos_gpu-dev\n",
    "\n",
    "# go into the submission directory \n",
    "cd ${SLURM_SUBMIT_DIR}\n",
    "\n",
    "# cleans out modules loaded in interactive and inherited by default\n",
    "module purge\n",
    "\n",
    "# loading modules\n",
    "module load pytorch-gpu/py3/1.7.0\n",
    "\n",
    "# echo of launched commands\n",
    "set -x\n",
    "\n",
    "# code execution\n",
    "srun python -u mnist-distributed.py --epochs 8 --batch-size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Soumission du script batch et affichage de la sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 210558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: IDRIS: setting exclusive mode for the job.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# submit job\n",
    "sbatch batch_multinode.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \n",
      "            210558   gpu_p13 mnist_py  ssos040  R       0:22      3 r11i4n[4-6] \n",
      " Done!\n"
     ]
    }
   ],
   "source": [
    "# watch Slurm queue line until the job is done\n",
    "# execution should take about 1 minute\n",
    "import time\n",
    "sq = !squeue -u $USER -n mnist_pytorch_multinode\n",
    "print(sq[0])\n",
    "while len(sq) >= 2:\n",
    "    print(sq[1],end='\\r')\n",
    "    time.sleep(5)\n",
    "    sq = !squeue -u $USER -n mnist_pytorch_multinode\n",
    "print('\\n Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pytorch-gpu/py3/1.7.0\r\n",
      "  Loading requirement: gcc/8.3.1 cuda/10.2 nccl/2.6.4-1-cuda\r\n",
      "    cudnn/7.6.5.32-cuda-10.2 intel-mkl/2020.1 magma/2.5.3-cuda\r\n",
      "    openmpi/4.0.2-cuda\r\n",
      "+ srun python -u mnist-distributed.py --epochs 8 --batch-size 128\r\n",
      "- Process 2 corresponds to GPU 2 of node 0\r\n",
      "- Process 3 corresponds to GPU 3 of node 0\r\n",
      ">>> Training on  3  nodes and  12  processes, master node is  r11i4n4\r\n",
      "- Process 0 corresponds to GPU 0 of node 0\r\n",
      "- Process 1 corresponds to GPU 1 of node 0\r\n",
      "- Process 4 corresponds to GPU 0 of node 1\r\n",
      "- Process 7 corresponds to GPU 3 of node 1\r\n",
      "- Process 5 corresponds to GPU 1 of node 1\r\n",
      "- Process 6 corresponds to GPU 2 of node 1\r\n",
      "- Process 10 corresponds to GPU 2 of node 2\r\n",
      "- Process 8 corresponds to GPU 0 of node 2\r\n",
      "- Process 9 corresponds to GPU 1 of node 2\r\n",
      "- Process 11 corresponds to GPU 3 of node 2\r\n",
      "Epoch [1/8], Step [200/500], Loss: 2.0609, Time data load: 1.045ms, Time training: 2.901ms\r\n",
      "Epoch [1/8], Step [400/500], Loss: 1.8782, Time data load: 1.040ms, Time training: 2.811ms\r\n",
      "Epoch [2/8], Step [200/500], Loss: 1.6847, Time data load: 1.038ms, Time training: 2.831ms\r\n",
      "Epoch [2/8], Step [400/500], Loss: 1.3931, Time data load: 1.037ms, Time training: 2.827ms\r\n",
      "Epoch [3/8], Step [200/500], Loss: 1.3893, Time data load: 1.041ms, Time training: 2.805ms\r\n",
      "Epoch [3/8], Step [400/500], Loss: 1.0682, Time data load: 1.043ms, Time training: 2.837ms\r\n",
      "Epoch [4/8], Step [200/500], Loss: 1.1825, Time data load: 1.041ms, Time training: 2.828ms\r\n",
      "Epoch [4/8], Step [400/500], Loss: 0.8613, Time data load: 1.039ms, Time training: 2.848ms\r\n",
      "Epoch [5/8], Step [200/500], Loss: 1.0350, Time data load: 1.045ms, Time training: 2.841ms\r\n",
      "Epoch [5/8], Step [400/500], Loss: 0.7263, Time data load: 1.042ms, Time training: 2.818ms\r\n",
      "Epoch [6/8], Step [200/500], Loss: 0.9254, Time data load: 1.025ms, Time training: 2.369ms\r\n",
      "Epoch [6/8], Step [400/500], Loss: 0.6328, Time data load: 1.035ms, Time training: 2.369ms\r\n",
      "Epoch [7/8], Step [200/500], Loss: 0.8389, Time data load: 1.032ms, Time training: 2.361ms\r\n",
      "Epoch [7/8], Step [400/500], Loss: 0.5629, Time data load: 1.035ms, Time training: 2.398ms\r\n",
      "Epoch [8/8], Step [200/500], Loss: 0.7696, Time data load: 1.028ms, Time training: 2.382ms\r\n",
      "Epoch [8/8], Step [400/500], Loss: 0.5084, Time data load: 1.029ms, Time training: 2.379ms\r\n",
      ">>> Training complete in: 0:00:16.416197\r\n"
     ]
    }
   ],
   "source": [
    "# display output\n",
    "%cat mnist_pytorch_multinode.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple d'exécution multi-nœuds à partir d'un checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Écriture du script batch de soumission\n",
    "\n",
    "**Rappel**:  si votre unique projet dispose d'heures CPU et GPU ou si votre login est rattaché à plusieurs projets, vous devez impérativement préciser l'attribution sur laquelle doit être décomptée les heures consommées par vos calculs, en ajoutant l'option `--account=my_project@gpu` comme indiqué dans la [documentation IDRIS](http://www.idris.fr/jean-zay/cpu/jean-zay-cpu-doc_account.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting batch_multinode.slurm\n"
     ]
    }
   ],
   "source": [
    "%%writefile batch_multinode.slurm\n",
    "#!/bin/sh\n",
    "#SBATCH --job-name=mnist_pytorch_multinode\n",
    "#SBATCH --output=mnist_pytorch_multinode.out\n",
    "#SBATCH --error=mnist_pytorch_multinode.out\n",
    "#SBATCH --nodes=3\n",
    "#SBATCH --gres=gpu:4\n",
    "#SBATCH --ntasks-per-node=4\n",
    "#SBATCH --cpus-per-task=10\n",
    "#SBATCH --hint=nomultithread\n",
    "#SBATCH --time=00:10:00\n",
    "#SBATCH --qos=qos_gpu-dev\n",
    "\n",
    "# go into the submission directory \n",
    "cd ${SLURM_SUBMIT_DIR}\n",
    "\n",
    "# cleans out modules loaded in interactive and inherited by default\n",
    "module purge\n",
    "\n",
    "# loading modules\n",
    "module load pytorch-gpu/py3/1.7.0\n",
    "\n",
    "# echo of launched commands\n",
    "set -x\n",
    "\n",
    "# code execution\n",
    "srun python -u mnist-distributed.py --epochs 8 --batch-size 128 -c ./checkpoint/12GPU_8epoch.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 210567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sbatch: IDRIS: setting exclusive mode for the job.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# submit job\n",
    "sbatch batch_multinode.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \n",
      "            210567   gpu_p13 mnist_py  ssos040  R       0:21      3 r11i4n[4-6] \n",
      " Done!\n"
     ]
    }
   ],
   "source": [
    "# watch Slurm queue line until the job is done\n",
    "# execution should take about 1 minute\n",
    "import time\n",
    "sq = !squeue -u $USER -n mnist_pytorch_multinode\n",
    "print(sq[0])\n",
    "while len(sq) >= 2:\n",
    "    print(sq[1],end='\\r')\n",
    "    time.sleep(5)\n",
    "    sq = !squeue -u $USER -n mnist_pytorch_multinode\n",
    "print('\\n Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pytorch-gpu/py3/1.7.0\r\n",
      "  Loading requirement: gcc/8.3.1 cuda/10.2 nccl/2.6.4-1-cuda\r\n",
      "    cudnn/7.6.5.32-cuda-10.2 intel-mkl/2020.1 magma/2.5.3-cuda\r\n",
      "    openmpi/4.0.2-cuda\r\n",
      "+ srun python -u mnist-distributed.py --epochs 8 --batch-size 128 -c ./checkpoint/12GPU_8epoch.checkpoint\r\n",
      "- Process 11 corresponds to GPU 3 of node 2\r\n",
      "- Process 7 corresponds to GPU 3 of node 1\r\n",
      "- Process 8 corresponds to GPU 0 of node 2\r\n",
      "- Process 9 corresponds to GPU 1 of node 2\r\n",
      "- Process 10 corresponds to GPU 2 of node 2\r\n",
      "- Process 3 corresponds to GPU 3 of node 0\r\n",
      "- Process 4 corresponds to GPU 0 of node 1\r\n",
      "- Process 5 corresponds to GPU 1 of node 1\r\n",
      "- Process 6 corresponds to GPU 2 of node 1\r\n",
      ">>> Training on  3  nodes and  12  processes, master node is  r11i4n4\r\n",
      "- Process 1 corresponds to GPU 1 of node 0\r\n",
      "- Process 0 corresponds to GPU 0 of node 0\r\n",
      "- Process 2 corresponds to GPU 2 of node 0\r\n",
      "Epoch [1/8], Step [200/500], Loss: 0.7129, Time data load: 1.039ms, Time training: 2.302ms\r\n",
      "Epoch [1/8], Step [400/500], Loss: 0.4636, Time data load: 1.047ms, Time training: 2.297ms\r\n",
      "Epoch [2/8], Step [200/500], Loss: 0.6651, Time data load: 1.044ms, Time training: 2.301ms\r\n",
      "Epoch [2/8], Step [400/500], Loss: 0.4265, Time data load: 1.048ms, Time training: 2.303ms\r\n",
      "Epoch [3/8], Step [200/500], Loss: 0.6245, Time data load: 1.034ms, Time training: 2.430ms\r\n",
      "Epoch [3/8], Step [400/500], Loss: 0.3945, Time data load: 1.032ms, Time training: 2.459ms\r\n",
      "Epoch [4/8], Step [200/500], Loss: 0.5895, Time data load: 1.031ms, Time training: 2.429ms\r\n",
      "Epoch [4/8], Step [400/500], Loss: 0.3668, Time data load: 1.038ms, Time training: 2.411ms\r\n",
      "Epoch [5/8], Step [200/500], Loss: 0.5587, Time data load: 1.040ms, Time training: 2.417ms\r\n",
      "Epoch [5/8], Step [400/500], Loss: 0.3427, Time data load: 1.034ms, Time training: 2.435ms\r\n",
      "Epoch [6/8], Step [200/500], Loss: 0.5309, Time data load: 1.026ms, Time training: 2.295ms\r\n",
      "Epoch [6/8], Step [400/500], Loss: 0.3214, Time data load: 1.037ms, Time training: 2.290ms\r\n",
      "Epoch [7/8], Step [200/500], Loss: 0.5059, Time data load: 1.030ms, Time training: 2.296ms\r\n",
      "Epoch [7/8], Step [400/500], Loss: 0.3022, Time data load: 1.029ms, Time training: 2.326ms\r\n",
      "Epoch [8/8], Step [200/500], Loss: 0.4834, Time data load: 1.038ms, Time training: 2.298ms\r\n",
      "Epoch [8/8], Step [400/500], Loss: 0.2852, Time data load: 1.027ms, Time training: 2.309ms\r\n",
      ">>> Training complete in: 0:00:15.353375\r\n"
     ]
    }
   ],
   "source": [
    "# display output\n",
    "%cat mnist_pytorch_multinode.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
