{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0faa010",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'container_abcs' from 'torch._six' (/gpfslocalsup/pub/anaconda-py3/2021.05/envs/pytorch-gpu-1.9.0+py3.9/lib/python3.9/site-packages/torch/_six.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4061299/2244796616.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvisualize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_decoder_arch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_encoder_arch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositionalencoding2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcustom_datasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs7kw/linkhome/rech/genkmw01/ueu39kt/cflow-ad/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFrEIA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mFf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFrEIA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mFm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/timm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_modules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_entrypoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mis_scriptable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_exportable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_scriptable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_exportable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/timm/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcspnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdensenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdla\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdpn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mefficientnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/timm/models/cspnet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMAGENET_DEFAULT_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_DEFAULT_STD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhelpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_model_with_cfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifierHead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConvBnAct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_norm_act_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/timm/models/helpers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeatureListNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFeatureDictNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFeatureHookNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2dSame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/timm/models/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mblur_pool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBlurPool2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClassifierHead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcond_conv2d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCondConv2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_condconv_initializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_exportable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_scriptable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_no_jit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_exportable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_scriptable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_no_jit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mset_layer_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/timm/models/layers/cond_conv2d.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhelpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_2tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconv2d_same\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv2d_same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_padding_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/timm/models/layers/helpers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_six\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'container_abcs' from 'torch._six' (/gpfslocalsup/pub/anaconda-py3/2021.05/envs/pytorch-gpu-1.9.0+py3.9/lib/python3.9/site-packages/torch/_six.py)"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, auc, precision_recall_curve\n",
    "from skimage.measure import label, regionprops\n",
    "from tqdm import tqdm\n",
    "from visualize import *\n",
    "from model import load_decoder_arch, load_encoder_arch, positionalencoding2d, activation\n",
    "from utils import *\n",
    "from custom_datasets import *\n",
    "from custom_models import *\n",
    "import pandas as pd\n",
    "gamma = 0.0\n",
    "theta = torch.nn.Sigmoid()\n",
    "log_theta = torch.nn.LogSigmoid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25216646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_meta_epoch_lnen(c, epoch, loader, encoder, decoders, pool_layers, N):\n",
    "    print('In test_meta_epoch_lnen')\n",
    "    # test\n",
    "    if c.verbose:\n",
    "        print('\\nCompute loss and scores on test set:')\n",
    "    #\n",
    "    P = c.condition_vec\n",
    "    decoders = [decoder.eval() for decoder in decoders]\n",
    "    height = list()\n",
    "    width = list()\n",
    "    image_list = list()\n",
    "    gt_label_list = list()\n",
    "    gt_mask_list = list()\n",
    "    test_loss = 0.0\n",
    "    test_count = 0\n",
    "    start = time.time()\n",
    "    files_path_list = []\n",
    "    loss_list = []\n",
    "    I = len(loader)\n",
    "    with torch.no_grad():\n",
    "        for i, (image, label, mask, filespath) in enumerate(tqdm(loader, disable=c.hide_tqdm_bar)):\n",
    "            print('image shape ', image.shape)\n",
    "            if i % 10 == 0:\n",
    "                print('step  % : ', (i/I) * 100, ' i/I = ', i , '/' , I)\n",
    "            files_path_list.append(filespath)\n",
    "            files_path_list_c = filespath\n",
    "            # save\n",
    "            if c.viz:\n",
    "                image_list.extend(t2np(image))\n",
    "            gt_label_list.extend(t2np(label))\n",
    "            gt_mask_list.extend(t2np(mask))\n",
    "            # data\n",
    "            image = image.to(c.device) # single scale\n",
    "            _ = encoder(image)  # BxCxHxW\n",
    "            # test decoder\n",
    "            e_list = list()\n",
    "            test_dist = [list() for layer in pool_layers]\n",
    "\n",
    "            for l, layer in enumerate(pool_layers):\n",
    "                if 'vit' in c.enc_arch:\n",
    "                    e = activation[layer].transpose(1, 2)[...,1:]\n",
    "                    e_hw = int(np.sqrt(e.size(2)))\n",
    "                    e = e.reshape(-1, e.size(1), e_hw, e_hw)  # BxCxHxW\n",
    "                else:\n",
    "                    e = activation[layer]  # BxCxHxW\n",
    "                #\n",
    "                B, C, H, W = e.size()\n",
    "                S = H*W\n",
    "                E = B*S\n",
    "                #\n",
    "                if i == 0:  # get stats\n",
    "                    height.append(H)\n",
    "                    width.append(W)\n",
    "                #\n",
    "                p = positionalencoding2d(P, H, W).to(c.device).unsqueeze(0).repeat(B, 1, 1, 1)\n",
    "                c_r = p.reshape(B, P, S).transpose(1, 2).reshape(E, P)  # BHWxP\n",
    "                e_r = e.reshape(B, C, S).transpose(1, 2).reshape(E, C)  # BHWxC\n",
    "                #\n",
    "                m = F.interpolate(mask, size=(H, W), mode='nearest')\n",
    "                m_r = m.reshape(B, 1, S).transpose(1, 2).reshape(E, 1)  # BHWx1\n",
    "                #\n",
    "                decoder = decoders[l]\n",
    "                FIB = E//N + int(E%N > 0)  # number of fiber batches\n",
    "                for f in range(FIB):\n",
    "                    if f < (FIB-1):\n",
    "                        idx = torch.arange(f*N, (f+1)*N)\n",
    "                    else:\n",
    "                        idx = torch.arange(f*N, E)\n",
    "                    #\n",
    "                    c_p = c_r[idx]  # NxP\n",
    "                    e_p = e_r[idx]  # NxC\n",
    "                    m_p = m_r[idx] > 0.5  # Nx1\n",
    "                    #\n",
    "                    if 'cflow' in c.dec_arch:\n",
    "                        z, log_jac_det = decoder(e_p, [c_p,])\n",
    "                    else:\n",
    "                        z, log_jac_det = decoder(e_p)\n",
    "                    #\n",
    "                    decoder_log_prob = get_logp(C, z, log_jac_det)\n",
    "                    log_prob = decoder_log_prob / C  # likelihood per dim\n",
    "                    loss = -log_theta(log_prob)\n",
    "                    test_loss += t2np(loss.sum())\n",
    "                    test_count += len(loss)\n",
    "                    test_dist[l] = test_dist[l] + log_prob.detach().cpu().tolist()\n",
    "\n",
    "                    #test_dist[l] =  log_prob.detach().cpu().tolist()\n",
    "                    #\n",
    "                print('test_dist  ', len(test_dist))\n",
    "                test_map = [list() for p in pool_layers]\n",
    "                print(' test_map len',  len(test_map) )\n",
    "                for l, p in enumerate(pool_layers):\n",
    "                    print(' l ' , l)\n",
    "                    test_norm = torch.tensor(test_dist[l], dtype=torch.double)  # EHWx1\n",
    "                    print('test_norm  ', test_norm.shape)\n",
    "                    print('height[l] ', height, 'width[l] ' , width)\n",
    "                    test_norm-= torch.max(test_norm) # normalize likelihoods to (-Inf:0] by subtracting a constant\n",
    "                    test_prob = torch.exp(test_norm) # convert to probs in range [0:1]\n",
    "                    print('test_prob ' , test_prob.shape)\n",
    "                    test_mask = test_prob.reshape(-1, height[l], width[l])\n",
    "                    print('test_mask 1 ', test_mask.shape)\n",
    "                    test_mask = test_prob.reshape(-1, height[l], width[l])\n",
    "                    print('test_mask 2 ', test_mask.shape)\n",
    "\n",
    "                    # upsample\n",
    "                    test_map[l] = F.interpolate(test_mask.unsqueeze(1),\n",
    "                        size=c.crp_size, mode='bilinear', align_corners=True).squeeze().numpy()\n",
    "                       # upsample\n",
    "                    print('test_map ',  test_map[l].shape)\n",
    "\n",
    "                # score aggregation\n",
    "                score_map = np.zeros_like(test_map[0])\n",
    "                print('  score_map' , score_map.shape)\n",
    "\n",
    "                for l, p in enumerate(pool_layers):\n",
    "                    score_map += test_map[l]\n",
    "                score_mask = score_map\n",
    "                # invert probs to anomaly scores\n",
    "                super_mask = score_mask.max() - score_mask\n",
    "                print('super_mask  ', super_mask.shape)\n",
    "                write_anom_map(c, super_mask, files_path_list_c)\n",
    "            \n",
    "            # del in memory\n",
    "#             del super_mask\n",
    "#             del score_mask\n",
    "#             del score_map\n",
    "#             del test_norm\n",
    "#             del test_prob\n",
    "#             del test_mask\n",
    "#             del test_map \n",
    "#             del test_dist\n",
    "            if i % 1000 == 0 :\n",
    "                print('Epoch: {:d} \\t step: {:.4f} '.format(epoch, i))\n",
    "    fps = len(loader.dataset) / (time.time() - start)\n",
    "    mean_test_loss = test_loss / test_count\n",
    "    if c.verbose:\n",
    "        print('Epoch: {:d} \\t test_loss: {:.4f} and {:.2f} fps'.format(epoch, mean_test_loss, fps))\n",
    "    #\n",
    "    return height, width, image_list, test_dist, gt_label_list, gt_mask_list, files_path_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053e9917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(c):\n",
    "    run_date = datetime.datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "    L = c.pool_layers # number of pooled layers\n",
    "    print('Number of pool layers =', L)\n",
    "    encoder, pool_layers, pool_dims = load_encoder_arch(c, L)\n",
    "    encoder = encoder.to(c.device).eval()\n",
    "    #print(encoder)\n",
    "    # NF decoder\n",
    "    decoders = [load_decoder_arch(c, pool_dim) for pool_dim in pool_dims]\n",
    "    decoders = [decoder.to(c.device) for decoder in decoders]\n",
    "    params = list(decoders[0].parameters())\n",
    "    for l in range(1, L):\n",
    "        params += list(decoders[l].parameters())\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.Adam(params, lr=c.lr)\n",
    "    # data\n",
    "    kwargs = {'num_workers': c.workers, 'pin_memory': True} if c.use_cuda else {}\n",
    "    # task data\n",
    "    if c.dataset == 'mvtec':\n",
    "        train_dataset = MVTecDataset(c, is_train=True)\n",
    "        test_dataset  = MVTecDataset(c, is_train=False)\n",
    "    elif c.dataset == 'stc':\n",
    "        train_dataset = StcDataset(c, is_train=True)\n",
    "        test_dataset  = StcDataset(c, is_train=False)\n",
    "    elif c.dataset == 'TumorNormal':\n",
    "        train_dataset = TumorNormalDataset(c, is_train=True)\n",
    "        test_dataset  = TumorNormalDataset(c, is_train=False)\n",
    "    else:\n",
    "        raise NotImplementedError('{} is not supported dataset!'.format(c.dataset))\n",
    "    #\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=c.batch_size, shuffle=True, drop_last=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=c.batch_size, shuffle=False, drop_last=False, **kwargs)\n",
    "    N = 256  # hyperparameter that increases batch size for the decoder model by N\n",
    "    print('train/test loader length', len(train_loader.dataset), len(test_loader.dataset))\n",
    "    print('train/test loader batches', len(train_loader), len(test_loader))\n",
    "    # stats\n",
    "    det_roc_obs = Score_Observer('DET_AUROC')\n",
    "    seg_roc_obs = Score_Observer('SEG_AUROC')\n",
    "    seg_pro_obs = Score_Observer('SEG_AUPRO')\n",
    "    if c.action_type == 'norm-test':\n",
    "        c.meta_epochs = 1\n",
    "    for epoch in range(c.meta_epochs):\n",
    "        if c.action_type == 'norm-test' and c.checkpoint:\n",
    "            load_weights(encoder, decoders, c.checkpoint)\n",
    "        elif c.action_type == 'norm-train':\n",
    "            print('Train meta epoch: {}'.format(epoch))\n",
    "            train_meta_epoch(c, epoch, train_loader, encoder, decoders, optimizer, pool_layers, N)\n",
    "            \n",
    "        else:\n",
    "            raise NotImplementedError('{} is not supported action type!'.format(c.action_type))\n",
    "        \n",
    "        #height, width, test_image_list, test_dist, gt_label_list, gt_mask_list = test_meta_fps(\n",
    "        #    c, epoch, test_loader, encoder, decoders, pool_layers, N)\n",
    "           \n",
    "        if c.dataset != 'TumorNormal' : \n",
    "            height, width, test_image_list, test_dist, gt_label_list, gt_mask_list, files_path_list = test_meta_epoch(\n",
    "            c, epoch, test_loader, encoder, decoders, pool_layers, N)\n",
    "        else:\n",
    "            height, width, test_image_list, test_dist, gt_label_list, gt_mask_list, files_path_list = test_meta_epoch_lnen(\n",
    "            c, epoch, test_loader, encoder, decoders, pool_layers, N)\n",
    "        files_path_list =  [item for sublist in files_path_list for item in sublist]\n",
    "\n",
    "        res_df = pd.DataFrame()\n",
    "        res_df['FilesPath'] = files_path_list\n",
    "        res_df['BinaryLabels'] = gt_label_list\n",
    "        # PxEHW\n",
    "        print('Heights/Widths', height, width)\n",
    "        test_map = [list() for p in pool_layers]\n",
    "        for l, p in enumerate(pool_layers):\n",
    "            test_norm = torch.tensor(test_dist[l], dtype=torch.double)  # EHWx1\n",
    "            test_norm-= torch.max(test_norm) # normalize likelihoods to (-Inf:0] by subtracting a constant\n",
    "            test_prob = torch.exp(test_norm) # convert to probs in range [0:1]\n",
    "            test_mask = test_prob.reshape(-1, height[l], width[l])\n",
    "            test_mask = test_prob.reshape(-1, height[l], width[l])\n",
    "            # upsample\n",
    "            test_map[l] = F.interpolate(test_mask.unsqueeze(1),\n",
    "                size=c.crp_size, mode='bilinear', align_corners=True).squeeze().numpy()\n",
    "        # score aggregation\n",
    "        score_map = np.zeros_like(test_map[0])\n",
    "        for l, p in enumerate(pool_layers):\n",
    "            score_map += test_map[l]\n",
    "        score_mask = score_map\n",
    "        # invert probs to anomaly scores\n",
    "        super_mask = score_mask.max() - score_mask\n",
    "\n",
    "        # calculate detection AUROC\n",
    "        score_label = np.max(super_mask, axis=(1, 2))\n",
    "        score_label_mean = np.mean(super_mask, axis=(1, 2))\n",
    "        res_df['MaxScoreAnomalyMap'] = score_label\n",
    "        res_df['MeanScoreAnomalyMap'] = score_label_mean\n",
    "        # Save result table \n",
    "        if c.action_type == 'norm-test':\n",
    "            export_results_df(c, res_df)\n",
    "            if  c.dataset != 'TumorNormal'  :\n",
    "                # Export anomaly map\n",
    "                write_anom_map(c, super_mask, files_path_list)\n",
    "\n",
    "        gt_label = np.asarray(gt_label_list, dtype=np.bool)\n",
    "        if not c.infer_train:\n",
    "            det_roc_auc = roc_auc_score(gt_label, score_label)\n",
    "            _ = det_roc_obs.update(100.0*det_roc_auc, epoch)\n",
    "            # calculate segmentation AUROC\n",
    "            if c.dataset != 'TumorNormal'  :\n",
    "                gt_mask = np.squeeze(np.asarray(gt_mask_list, dtype=np.bool), axis=1)\n",
    "                seg_roc_auc = roc_auc_score(gt_mask.flatten(), super_mask.flatten())\n",
    "                save_best_seg_weights = seg_roc_obs.update(100.0*seg_roc_auc, epoch)\n",
    "                if save_best_seg_weights and c.action_type != 'norm-test':\n",
    "                    save_weights(c, encoder, decoders, c.model, run_date)  # avoid unnecessary saves\n",
    "            # calculate segmentation AUPRO\n",
    "            # from https://github.com/YoungGod/DFR:\n",
    "            # No mask for TunorNormal -> Results not meaningful\n",
    "            if c.pro and  c.dataset != 'TumorNormal' :  # and (epoch % 4 == 0):  # AUPRO is expensive to compute\n",
    "                max_step = 1000\n",
    "                expect_fpr = 0.3  # default 30%\n",
    "                max_th = super_mask.max()\n",
    "                min_th = super_mask.min()\n",
    "                delta = (max_th - min_th) / max_step\n",
    "                ious_mean = []\n",
    "                ious_std = []\n",
    "                pros_mean = []\n",
    "                pros_std = []\n",
    "                threds = []\n",
    "                fprs = []\n",
    "                binary_score_maps = np.zeros_like(super_mask, dtype=np.bool)\n",
    "                for step in range(max_step):\n",
    "                    thred = max_th - step * delta\n",
    "                    # segmentation\n",
    "                    binary_score_maps[super_mask <= thred] = 0\n",
    "                    binary_score_maps[super_mask >  thred] = 1\n",
    "                    pro = []  # per region overlap\n",
    "                    iou = []  # per image iou\n",
    "                    # pro: find each connected gt region, compute the overlapped pixels between the gt region and predicted region\n",
    "                    # iou: for each image, compute the ratio, i.e. intersection/union between the gt and predicted binary map \n",
    "                    for i in range(len(binary_score_maps)):    # for i th image\n",
    "                        # pro (per region level)\n",
    "                        label_map = label(gt_mask[i], connectivity=2)\n",
    "                        props = regionprops(label_map)\n",
    "                        for prop in props:\n",
    "                            x_min, y_min, x_max, y_max = prop.bbox    # find the bounding box of an anomaly region \n",
    "                            cropped_pred_label = binary_score_maps[i][x_min:x_max, y_min:y_max]\n",
    "                            # cropped_mask = gt_mask[i][x_min:x_max, y_min:y_max]   # bug!\n",
    "                            cropped_mask = prop.filled_image    # corrected!\n",
    "                            intersection = np.logical_and(cropped_pred_label, cropped_mask).astype(np.float32).sum()\n",
    "                            pro.append(intersection / prop.area)\n",
    "                        # iou (per image level)\n",
    "                        intersection = np.logical_and(binary_score_maps[i], gt_mask[i]).astype(np.float32).sum()\n",
    "                        union = np.logical_or(binary_score_maps[i], gt_mask[i]).astype(np.float32).sum()\n",
    "                        if gt_mask[i].any() > 0:    # when the gt have no anomaly pixels, skip it\n",
    "                            iou.append(intersection / union)\n",
    "                    # against steps and average metrics on the testing data\n",
    "                    ious_mean.append(np.array(iou).mean())\n",
    "                    #print(\"per image mean iou:\", np.array(iou).mean())\n",
    "                    ious_std.append(np.array(iou).std())\n",
    "                    pros_mean.append(np.array(pro).mean())\n",
    "                    pros_std.append(np.array(pro).std())\n",
    "                    # fpr for pro-auc\n",
    "                    gt_masks_neg = ~gt_mask\n",
    "                    fpr = np.logical_and(gt_masks_neg, binary_score_maps).sum() / gt_masks_neg.sum()\n",
    "                    fprs.append(fpr)\n",
    "                    threds.append(thred)\n",
    "                # as array\n",
    "                threds = np.array(threds)\n",
    "                pros_mean = np.array(pros_mean)\n",
    "                pros_std = np.array(pros_std)\n",
    "                fprs = np.array(fprs)\n",
    "                ious_mean = np.array(ious_mean)\n",
    "                ious_std = np.array(ious_std)\n",
    "                # best per image iou\n",
    "                best_miou = ious_mean.max()\n",
    "                #print(f\"Best IOU: {best_miou:.4f}\")\n",
    "                # default 30% fpr vs pro, pro_auc\n",
    "                idx = fprs <= expect_fpr  # find the indexs of fprs that is less than expect_fpr (default 0.3)\n",
    "                fprs_selected = fprs[idx]\n",
    "                fprs_selected = rescale(fprs_selected)  # rescale fpr [0,0.3] -> [0, 1]\n",
    "                pros_mean_selected = pros_mean[idx]    \n",
    "                seg_pro_auc = auc(fprs_selected, pros_mean_selected)\n",
    "                _ = seg_pro_obs.update(100.0*seg_pro_auc, epoch)\n",
    "        #\n",
    "        if  c.dataset != 'TumorNormal':\n",
    "            save_results(c, det_roc_obs, seg_roc_obs, seg_pro_obs, c.model, c.class_name, run_date)\n",
    "        # export visualuzations\n",
    "        if c.viz  and  c.dataset != 'TumorNormal' :\n",
    "            precision, recall, thresholds = precision_recall_curve(gt_label, score_label)\n",
    "            a = 2 * precision * recall\n",
    "            b = precision + recall\n",
    "            f1 = np.divide(a, b, out=np.zeros_like(a), where=b != 0)\n",
    "            det_threshold = thresholds[np.argmax(f1)]\n",
    "            print('Optimal DET Threshold: {:.2f}'.format(det_threshold))\n",
    "            precision, recall, thresholds = precision_recall_curve(gt_mask.flatten(), super_mask.flatten())\n",
    "            a = 2 * precision * recall\n",
    "            b = precision + recall\n",
    "            f1 = np.divide(a, b, out=np.zeros_like(a), where=b != 0)\n",
    "            seg_threshold = thresholds[np.argmax(f1)]\n",
    "            print('Optimal SEG Threshold: {:.2f}'.format(seg_threshold))\n",
    "            export_groundtruth(c, test_image_list, gt_mask)\n",
    "            export_scores(c, test_image_list, super_mask, seg_threshold)\n",
    "            export_test_images(c, test_image_list, gt_mask, super_mask, seg_threshold)\n",
    "            export_hist(c, gt_mask, super_mask, seg_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd97b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
